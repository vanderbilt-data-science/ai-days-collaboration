{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zzS3PObgd6yK",
        "Z4-iV3WNTkP8",
        "-wEYQKLvT0dR",
        "sBBySnEqYv8e",
        "ONuCvVMLUQxG",
        "uF1gdOVee62a",
        "m7n-wHpJU9RN",
        "gJuzU5EGVJKr",
        "0au4wjdIWQMo",
        "Ogaz4b-Je_58",
        "Cgb5sLzIW49K",
        "eNZ9lY-fXGt9"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7qRkZLZCQ0KOcq6+B0NL1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80dab79d8a454d7d84a32a32585a794b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e36774101a1d4d698b6b796398706c2c",
              "IPY_MODEL_9f835613d90e4c31a5b26e7b15b13279",
              "IPY_MODEL_9db2d09dfef84865945c79ff92b01875"
            ],
            "layout": "IPY_MODEL_66315d45739147d39d90547e1056606b"
          }
        },
        "e36774101a1d4d698b6b796398706c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac3b5dc7f804398801fc51830557aea",
            "placeholder": "​",
            "style": "IPY_MODEL_f5fa9e540c014b48b71c7254f9fba549",
            "value": "100%"
          }
        },
        "9f835613d90e4c31a5b26e7b15b13279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_573b3ceb471e4fbfb160a336a5aedcd7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c38be5d53147aa80e139a524b15c5d",
            "value": 2
          }
        },
        "9db2d09dfef84865945c79ff92b01875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66027f444241400eac14c4667787db00",
            "placeholder": "​",
            "style": "IPY_MODEL_434b6f04f7ca43b0ae1d5d1b0f5c2bb2",
            "value": " 2/2 [00:00&lt;00:00,  2.32it/s]"
          }
        },
        "66315d45739147d39d90547e1056606b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac3b5dc7f804398801fc51830557aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5fa9e540c014b48b71c7254f9fba549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "573b3ceb471e4fbfb160a336a5aedcd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c38be5d53147aa80e139a524b15c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66027f444241400eac14c4667787db00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434b6f04f7ca43b0ae1d5d1b0f5c2bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai-days-collaboration/blob/main/Process_cvs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Days Collaboration Matcher\n",
        "# ===========================\n",
        "This notebook extracts research profiles from CVs and research statements using Claude 3.7 Sonnet, structures the information in JSON format, and prepares data for collaboration matching."
      ],
      "metadata": {
        "id": "Wiu_wp7kTFg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Initial Setup\n",
        "Run this section once at the beginning of your session to set up the environment, import libraries, and connect to Google Drive.\n"
      ],
      "metadata": {
        "id": "NXtmRkTGTPo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Mount Google Drive\n",
        "This connects your Google Drive to access files and save results."
      ],
      "metadata": {
        "id": "zzS3PObgd6yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgvsuEySTef7",
        "outputId": "5fa08753-432a-45a2-b3f4-1dd4efeb2313"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Install Required Packages\n",
        "These packages are needed for processing documents and using the Claude API.\n"
      ],
      "metadata": {
        "id": "Z4-iV3WNTkP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8o04r9AmNt1E",
        "outputId": "4940e0d0-ae21-4be4-81c1-d6f436f0f07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.49.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install anthropic PyPDF2 python-docx pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wI7c5f0lTEPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Import Libraries\n",
        "Import all necessary libraries for file processing, data manipulation, and API interaction.\n"
      ],
      "metadata": {
        "id": "-wEYQKLvT0dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import anthropic\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uu05wwYLrcF0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Define Paths and Constants\n",
        "Configure file locations and other important settings."
      ],
      "metadata": {
        "id": "sBBySnEqYv8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for multiple input directories\n",
        "INPUT_DIRS = [\n",
        "    \"/content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research\",\n",
        "    \"/content/drive/My Drive/Data Science/Symposium/Recent_CV_uploads\"\n",
        "]\n",
        "\n",
        "# Output directory for saving profiles\n",
        "OUTPUT_PATH = \"/content/drive/My Drive/Data Science/Symposium/AI Days Output\"\n",
        "OUTPUT_SUMMARY_FILE = os.path.join(OUTPUT_PATH, \"all_profiles.json\")\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "# Google Sheet information for form responses\n",
        "FORM_RESPONSES_SHEET_ID = \"1347yg-cKPv3VWmXSg6M7X-AI4QDCDETQUBPaxXWpfmc\"\n",
        "FORM_RESPONSES_RANGE = \"Form Responses 1!A:Z\"  # This gets all columns, we'll filter to what we need\n",
        "\n",
        "# Column name for research statements in the form\n",
        "RESEARCH_STATEMENT_COLUMN = \"OPTION 2: Paste your Information\\n\\nPaste your professional summary and interests below (max 500 words) \"\n",
        "\n",
        "# Column names for identifying information\n",
        "EMAIL_COLUMN = \"Email\"\n",
        "FIRST_NAME_COLUMN = \"First Name\"\n",
        "LAST_NAME_COLUMN = \"Last Name\"\n",
        "AFFILIATION_COLUMN = \"Affiliation\"  # Note the space at the end\n"
      ],
      "metadata": {
        "id": "QcPbJQh6Y0tP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Set Up API Key\n",
        "Configure access to the Anthropic Claude API."
      ],
      "metadata": {
        "id": "ONuCvVMLUQxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Anthropic API key\n",
        "from google.colab import userdata\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n"
      ],
      "metadata": {
        "id": "WjQt7NgBUjvf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Core Functions\n",
        "This section contains all the core functions for processing documents, extracting text, and generating research profiles using Claude 3.7 Sonnet.\n"
      ],
      "metadata": {
        "id": "TZprZ2fHUlEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2.1 File Processing Functions\n",
        "Functions to extract text from various document formats (PDF, DOCX, TXT)."
      ],
      "metadata": {
        "id": "uF1gdOVee62a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extract text from PDF files\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting PDF {file_path}: {str(e)}\")\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Extract text from DOCX files\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting DOCX {file_path}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"Extract text from various file formats\"\"\"\n",
        "    _, ext = os.path.splitext(file_path)\n",
        "    ext = ext.lower()\n",
        "\n",
        "    try:\n",
        "        if ext == '.pdf':\n",
        "            return extract_text_from_pdf(file_path)\n",
        "        elif ext in ['.docx', '.doc']:\n",
        "            return extract_text_from_docx(file_path)\n",
        "        elif ext == '.txt':\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            print(f\"Unsupported file format: {ext}\")\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {file_path}: {str(e)}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "TSVf95S9U44_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Research Profile Extraction\n",
        "Function to use Claude 3.7 Sonnet to extract structured information from documents.\n"
      ],
      "metadata": {
        "id": "m7n-wHpJU9RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_research_profile(text, file_name):\n",
        "    \"\"\"\n",
        "    Extract research profile using Claude 3.7 Sonnet in structured JSON format\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with structured research profile information\n",
        "    \"\"\"\n",
        "    # Create system prompt\n",
        "    system_prompt = \"\"\"You are an expert assistant helping to extract structured information from academic resumes and research statements.\n",
        "    Extract the information according to the specified JSON format, ensuring all fields are included with the correct structure.\"\"\"\n",
        "\n",
        "    # Create user prompt with structured JSON format\n",
        "    user_prompt = f\"\"\"\n",
        "    Analyze this document and extract key research information in the following JSON format:\n",
        "\n",
        "    {{\n",
        "      \"basic_info\": {{\n",
        "        \"name\": \"Full name of the researcher\",\n",
        "        \"email\": \"Email address if available, otherwise null\",\n",
        "        \"affiliation\": \"University or organization name\",\n",
        "        \"role\": \"The person's role (faculty, physician-scientist, clinical_faculty, clinician_in_training, postdoc, research_scientist, graduate_student, undergraduate_student, research_staff, industry, community, or other)\"\n",
        "      }},\n",
        "      \"research_profile\": {{\n",
        "        \"primary_focus\": \"A 1-2 sentence description of their main research area\",\n",
        "        \"methodologies\": [\"Method 1\", \"Method 2\", \"Method 3\"],\n",
        "        \"domains\": [\"Application domain 1\", \"Application domain 2\"],\n",
        "        \"research_summary\": \"A 150-200 word paragraph describing their research in detail\"\n",
        "      }},\n",
        "      \"collaboration_potential\": {{\n",
        "        \"expertise_offered\": [\"Specific expertise 1\", \"Specific expertise 2\"],\n",
        "        \"resources_available\": [\"Dataset\", \"Tool\", \"Framework\"],\n",
        "        \"complementary_fields\": [\"Field 1\", \"Field 2\"]\n",
        "      }},\n",
        "      \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"]\n",
        "    }}\n",
        "\n",
        "    Guidelines:\n",
        "    1. Extract information explicitly stated in the document when available\n",
        "    2. Make reasonable inferences for missing fields, marking these as \"[inferred]\"\n",
        "    3. Use \"null\" for fields where no information is available and no inference can be made\n",
        "    4. Limit lists to 3-5 items, prioritizing the most significant ones\n",
        "    5. Ensure the research_summary provides a coherent overview of their work\n",
        "    6. Focus on information most relevant for identifying potential research collaborations\n",
        "    7. For the \"role\" field, determine the most appropriate category:\n",
        "       - faculty: Academic professors and researchers without significant clinical duties\n",
        "       - physician-scientist: Medical doctors who conduct significant research\n",
        "       - clinical_faculty: Primarily clinical practitioners with academic appointments\n",
        "       - clinician_in_training: Residents, fellows, and others in clinical training positions\n",
        "       - postdoc: Postdoctoral researchers\n",
        "       - research_scientist: Non-faculty research professionals\n",
        "       - graduate_student: Master's and PhD students\n",
        "       - undergraduate_student: Bachelor's degree students\n",
        "       - research_staff: Research assistants, lab managers, etc.\n",
        "       - industry: Private sector professionals\n",
        "       - community: Community organization members\n",
        "       - other: Roles that don't fit the above categories\n",
        "\n",
        "    The JSON structure must be strictly followed without additional narrative or explanation outside the JSON object.\n",
        "\n",
        "    Document:\n",
        "    {text[:15000]}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Call Anthropic API with Claude 3.7 Sonnet\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=1000,\n",
        "            system=system_prompt,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        # Extract and parse the JSON response\n",
        "        response_text = response.content[0].text\n",
        "\n",
        "        # Find JSON in the response (in case Claude adds any commentary)\n",
        "        json_match = re.search(r'({.*})', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            profile = json.loads(json_str)\n",
        "\n",
        "            # Add file information\n",
        "            profile['file_info'] = {\n",
        "                'file_name': file_name,\n",
        "                'processing_time': pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "\n",
        "            return profile\n",
        "        else:\n",
        "            # If no JSON formatting, return an error structure\n",
        "            return {\n",
        "                \"basic_info\": {\n",
        "                    \"name\": \"Error parsing\",\n",
        "                    \"email\": \"Error parsing\",\n",
        "                    \"affiliation\": \"Error parsing\",\n",
        "                    \"role\": \"Error parsing\"\n",
        "                },\n",
        "                \"research_profile\": {\n",
        "                    \"primary_focus\": \"Error parsing JSON from response\",\n",
        "                    \"methodologies\": [],\n",
        "                    \"domains\": [],\n",
        "                    \"research_summary\": response_text\n",
        "                },\n",
        "                \"collaboration_potential\": {\n",
        "                    \"expertise_offered\": [],\n",
        "                    \"resources_available\": [],\n",
        "                    \"complementary_fields\": []\n",
        "                },\n",
        "                \"keywords\": [],\n",
        "                \"file_info\": {\n",
        "                    \"file_name\": file_name,\n",
        "                    \"processing_time\": pd.Timestamp.now().isoformat(),\n",
        "                    \"error\": \"Failed to parse JSON from response\"\n",
        "                }\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting research profile from {file_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"basic_info\": {\n",
        "                \"name\": \"Error\",\n",
        "                \"email\": \"Error\",\n",
        "                \"affiliation\": \"Error\",\n",
        "                \"role\": \"Error\"\n",
        "            },\n",
        "            \"research_profile\": {\n",
        "                \"primary_focus\": \"Error processing\",\n",
        "                \"methodologies\": [],\n",
        "                \"domains\": [],\n",
        "                \"research_summary\": f\"Error: {str(e)}\"\n",
        "            },\n",
        "            \"collaboration_potential\": {\n",
        "                \"expertise_offered\": [],\n",
        "                \"resources_available\": [],\n",
        "                \"complementary_fields\": []\n",
        "            },\n",
        "            \"keywords\": [],\n",
        "            \"file_info\": {\n",
        "                \"file_name\": file_name,\n",
        "                \"processing_time\": pd.Timestamp.now().isoformat(),\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "        }\n"
      ],
      "metadata": {
        "id": "oo4aeyE7VCwS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Directory Scanning Functions\n",
        "Functions to scan multiple directories and get file lists.\n"
      ],
      "metadata": {
        "id": "gJuzU5EGVJKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scan_directory(directory, limit=None):\n",
        "    \"\"\"\n",
        "    Scan a single directory for files\n",
        "\n",
        "    Args:\n",
        "        directory: Path to scan\n",
        "        limit: Optional limit on number of files to return\n",
        "\n",
        "    Returns:\n",
        "        List of file paths\n",
        "    \"\"\"\n",
        "    file_paths = []\n",
        "\n",
        "    try:\n",
        "        # Walk through the directory\n",
        "        for root, _, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                # Only include document files\n",
        "                _, ext = os.path.splitext(file)\n",
        "                if ext.lower() in ['.pdf', '.docx', '.doc', '.txt']:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_paths.append(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scanning directory {directory}: {str(e)}\")\n",
        "\n",
        "    # Limit the number of files if specified\n",
        "    if limit and len(file_paths) > limit:\n",
        "        file_paths = file_paths[:limit]\n",
        "\n",
        "    return file_paths\n",
        "\n",
        "def scan_all_directories(directories, limit=None):\n",
        "    \"\"\"\n",
        "    Scan multiple directories and combine results\n",
        "\n",
        "    Args:\n",
        "        directories: List of directory paths to scan\n",
        "        limit: Optional limit on total number of files to return\n",
        "\n",
        "    Returns:\n",
        "        List of file paths\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    for directory in directories:\n",
        "        print(f\"Scanning directory: {directory}\")\n",
        "        files = scan_directory(directory)\n",
        "        print(f\"Found {len(files)} files\")\n",
        "        all_files.extend(files)\n",
        "\n",
        "    # Limit the total number of files if specified\n",
        "    if limit and len(all_files) > limit:\n",
        "        all_files = all_files[:limit]\n",
        "\n",
        "    print(f\"Total files found: {len(all_files)}\")\n",
        "    return all_files"
      ],
      "metadata": {
        "id": "MYb4rZEvWKza"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Profile Display Function\n",
        "Function to display extracted profiles in a readable format."
      ],
      "metadata": {
        "id": "0au4wjdIWQMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_profiles(profiles, limit=None):\n",
        "    \"\"\"Display the extracted profiles in a readable format\"\"\"\n",
        "    # Limit number of profiles to display if specified\n",
        "    if limit and len(profiles) > limit:\n",
        "        display_profiles = profiles[:limit]\n",
        "        print(f\"Displaying {limit} of {len(profiles)} profiles\")\n",
        "    else:\n",
        "        display_profiles = profiles\n",
        "\n",
        "    for i, profile in enumerate(display_profiles):\n",
        "        print(f\"\\n{'='*80}\\nProfile {i+1}: {profile.get('file_info', {}).get('file_name', 'Unknown')}\\n{'='*80}\")\n",
        "\n",
        "        # Basic Info\n",
        "        basic_info = profile.get('basic_info', {})\n",
        "        print(f\"Name: {basic_info.get('name', 'Not extracted')}\")\n",
        "        print(f\"Email: {basic_info.get('email', 'Not extracted')}\")\n",
        "        print(f\"Affiliation: {basic_info.get('affiliation', 'Not extracted')}\")\n",
        "        print(f\"Role: {basic_info.get('role', 'Not extracted')}\")\n",
        "\n",
        "        # Research Profile\n",
        "        research_profile = profile.get('research_profile', {})\n",
        "        print(f\"\\nPrimary Focus: {research_profile.get('primary_focus', 'Not extracted')}\")\n",
        "\n",
        "        print(\"\\nMethodologies:\")\n",
        "        for method in research_profile.get('methodologies', []):\n",
        "            print(f\"- {method}\")\n",
        "\n",
        "        print(\"\\nDomains:\")\n",
        "        for domain in research_profile.get('domains', []):\n",
        "            print(f\"- {domain}\")\n",
        "\n",
        "        print(f\"\\nResearch Summary:\\n{research_profile.get('research_summary', 'Not extracted')}\")\n",
        "\n",
        "        # Collaboration Potential\n",
        "        collab = profile.get('collaboration_potential', {})\n",
        "        print(\"\\nExpertise Offered:\")\n",
        "        for expertise in collab.get('expertise_offered', []):\n",
        "            print(f\"- {expertise}\")\n",
        "\n",
        "        print(\"\\nResources Available:\")\n",
        "        for resource in collab.get('resources_available', []):\n",
        "            print(f\"- {resource}\")\n",
        "\n",
        "        print(\"\\nComplementary Fields:\")\n",
        "        for field in collab.get('complementary_fields', []):\n",
        "            print(f\"- {field}\")\n",
        "\n",
        "        # Keywords\n",
        "        print(\"\\nKeywords:\")\n",
        "        for keyword in profile.get('keywords', []):\n",
        "            print(f\"- {keyword}\")\n",
        "\n",
        "    if limit and len(profiles) > limit:\n",
        "        print(f\"\\n... and {len(profiles) - limit} more profiles (not displayed)\")"
      ],
      "metadata": {
        "id": "TQL8qSSVWaFS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Form Response Processing Functions\n",
        "Functions to load and process text-based research statements from responses."
      ],
      "metadata": {
        "id": "FLwKQPFrtgvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "\n",
        "def load_form_responses(sheet_id, range_name):\n",
        "    \"\"\"\n",
        "    Load form responses from Google Sheets with special handling for multiline headers\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "    from google.auth import default\n",
        "    import pandas as pd\n",
        "\n",
        "    print(f\"Loading form responses from sheet ID: {sheet_id}\")\n",
        "\n",
        "    try:\n",
        "        # Authenticate and build the service\n",
        "        creds, _ = default()\n",
        "        service = build('sheets', 'v4', credentials=creds)\n",
        "\n",
        "        # Call the Sheets API to get raw values\n",
        "        sheet = service.spreadsheets()\n",
        "        result = sheet.values().get(spreadsheetId=sheet_id, range=range_name).execute()\n",
        "        values = result.get('values', [])\n",
        "\n",
        "        if not values:\n",
        "            print(\"No data found in the sheet.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Print raw headers to help with debugging\n",
        "        headers = values[0]\n",
        "        print(\"Raw headers from API:\")\n",
        "        for i, header in enumerate(headers):\n",
        "            print(f\"  {i}: {header}\")\n",
        "\n",
        "        # Get research statement column index (usually column K = index 10)\n",
        "        research_col_index = 10  # Default to column K\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(values[1:], columns=headers)\n",
        "\n",
        "        # Handle research statement column directly from raw data\n",
        "        if len(headers) > research_col_index:\n",
        "            research_header = headers[research_col_index]\n",
        "            print(f\"Using research statement column: '{research_header}'\")\n",
        "\n",
        "            # Add a simpler column name that's easier to reference\n",
        "            df['research_statement'] = None\n",
        "\n",
        "            # Directly copy values from the raw data\n",
        "            for i, row in enumerate(values[1:]):\n",
        "                if len(row) > research_col_index and row[research_col_index]:\n",
        "                    df.at[i, 'research_statement'] = row[research_col_index]\n",
        "\n",
        "        print(f\"Loaded {len(df)} form responses\")\n",
        "\n",
        "        # Count non-empty research statements\n",
        "        if 'research_statement' in df.columns:\n",
        "            non_empty = df['research_statement'].notna().sum()\n",
        "            print(f\"Found {non_empty} non-empty research statements\")\n",
        "\n",
        "            # Show examples\n",
        "            if non_empty > 0:\n",
        "                examples = df[df['research_statement'].notna()].head(3)\n",
        "                print(\"\\nExamples of research statements:\")\n",
        "                for i, row in examples.iterrows():\n",
        "                    text = row['research_statement']\n",
        "                    preview = text[:100] + \"...\" if len(text) > 100 else text\n",
        "                    print(f\"  Row {i+1}: {preview}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading form responses: {str(e)}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def extract_profile_from_text_response(row, research_column, email_column, first_name_column, last_name_column):\n",
        "    \"\"\"\n",
        "    Extract research profile from text-based response using Claude\n",
        "\n",
        "    Args:\n",
        "        row: Row from form responses DataFrame\n",
        "        research_column: Column name containing the research statement\n",
        "        email_column: Column name containing email address\n",
        "        first_name_column: Column name containing respondent's first name\n",
        "        last_name_column: Column name containing respondent's last name\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with structured research profile information\n",
        "    \"\"\"\n",
        "    # Get identification info\n",
        "    email = row.get(email_column, '')\n",
        "    first_name = row.get(first_name_column, '')\n",
        "    last_name = row.get(last_name_column, '')\n",
        "    full_name = f\"{first_name} {last_name}\".strip()\n",
        "\n",
        "    # Get research statement text\n",
        "    text = row.get(research_column, '')\n",
        "\n",
        "    if not text or str(text).strip() == '':\n",
        "        print(f\"No text found for {full_name} ({email})\")\n",
        "        return None\n",
        "\n",
        "    # Use the same extraction function we use for files\n",
        "    print(f\"Extracting profile for {full_name} ({email})\")\n",
        "    profile = extract_research_profile(text, f\"Form Response - {full_name}\")\n",
        "\n",
        "    # Add form response info\n",
        "    profile['form_info'] = {\n",
        "        'email': email,\n",
        "        'first_name': first_name,\n",
        "        'last_name': last_name,\n",
        "        'full_name': full_name,\n",
        "        'response_type': 'text',\n",
        "        'processing_time': pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "def process_form_responses(sheet_id, range_name, email_column, first_name_column, last_name_column, processed_emails=None):\n",
        "    \"\"\"\n",
        "    Process all form responses with text-based research statements\n",
        "\n",
        "    Args:\n",
        "        sheet_id: The ID of the Google Sheet\n",
        "        range_name: The range to load\n",
        "        email_column: Column name containing email addresses\n",
        "        first_name_column: Column name containing first names\n",
        "        last_name_column: Column name containing last names\n",
        "        processed_emails: Set of emails that have already been processed\n",
        "\n",
        "    Returns:\n",
        "        List of extracted profiles\n",
        "    \"\"\"\n",
        "    # Initialize processed_emails if not provided\n",
        "    if processed_emails is None:\n",
        "        processed_emails = set()\n",
        "    else:\n",
        "        print(f\"Skipping {len(processed_emails)} already processed form submissions\")\n",
        "\n",
        "    # Load form responses with special handling for research statements\n",
        "    df = load_form_responses(sheet_id, range_name)\n",
        "\n",
        "    if df.empty:\n",
        "        return []\n",
        "\n",
        "    # Track emails processed in this run to avoid duplicates\n",
        "    currently_processed = set()\n",
        "    profiles = []\n",
        "\n",
        "    print(f\"Processing {len(df)} form responses\")\n",
        "\n",
        "    # Use the simplified 'research_statement' column\n",
        "    research_column = 'research_statement'\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        try:\n",
        "            # Extract email for identification\n",
        "            email = row.get(email_column, '')\n",
        "\n",
        "            # Skip if no email (unlikely but possible)\n",
        "            if not email:\n",
        "                print(f\"Skipping row {i+1}: No email address found\")\n",
        "                continue\n",
        "\n",
        "            # Skip if already processed in a previous run\n",
        "            if email in processed_emails:\n",
        "                print(f\"Skipping previously processed submission from {email}\")\n",
        "                continue\n",
        "\n",
        "            # Skip if already processed in this run (duplicate in sheet)\n",
        "            if email in currently_processed:\n",
        "                print(f\"Skipping duplicate entry for {email}\")\n",
        "                continue\n",
        "\n",
        "            # Check if this row has a text-based research statement\n",
        "            if research_column in row and row[research_column] and isinstance(row[research_column], str) and row[research_column].strip():\n",
        "                profile = extract_profile_from_text_response(row, research_column, email_column, first_name_column, last_name_column)\n",
        "\n",
        "                if profile:\n",
        "                    # Save individual profile\n",
        "                    safe_email = email.replace('@', '_at_').replace('.', '_dot_')\n",
        "                    output_file = os.path.join(OUTPUT_PATH, f\"form_response_{safe_email}_profile.json\")\n",
        "                    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(profile, f, indent=2)\n",
        "\n",
        "                    profiles.append(profile)\n",
        "                    currently_processed.add(email)\n",
        "                    print(f\"Processed form response {i+1}/{len(df)} for {email}\")\n",
        "            else:\n",
        "                print(f\"Skipping row {i+1}: No research statement text found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {i+1}: {str(e)}\")\n",
        "\n",
        "    print(f\"Extracted {len(profiles)} profiles from form responses\")\n",
        "    return profiles"
      ],
      "metadata": {
        "id": "jLabwY0KtzR-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Processing Functions\n",
        "Functions for tracking processed files, estimating runtime, and handling incremental updates.\n"
      ],
      "metadata": {
        "id": "1N8n860JWewu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.1 File Tracking Functions\n",
        "Functions to track processed files and identify new submissions."
      ],
      "metadata": {
        "id": "Ogaz4b-Je_58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_existing_profiles():\n",
        "    \"\"\"\n",
        "    Load existing profiles from the output summary file\n",
        "\n",
        "    Returns:\n",
        "        List of profiles, a set of processed file paths, and a set of processed emails\n",
        "    \"\"\"\n",
        "    profiles = []\n",
        "    processed_files = set()\n",
        "    processed_emails = set()\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(OUTPUT_SUMMARY_FILE):\n",
        "            with open(OUTPUT_SUMMARY_FILE, 'r', encoding='utf-8') as f:\n",
        "                profiles = json.load(f)\n",
        "\n",
        "            # Extract file paths and emails from the loaded profiles\n",
        "            for profile in profiles:\n",
        "                # Track file-based profiles\n",
        "                if 'file_info' in profile and 'file_path' in profile['file_info']:\n",
        "                    processed_files.add(profile['file_info']['file_path'])\n",
        "\n",
        "                # Track form-based profiles by email\n",
        "                if 'form_info' in profile and 'email' in profile['form_info']:\n",
        "                    processed_emails.add(profile['form_info']['email'])\n",
        "\n",
        "            print(f\"Loaded {len(profiles)} existing profiles\")\n",
        "            print(f\"Found {len(processed_files)} previously processed files\")\n",
        "            print(f\"Found {len(processed_emails)} previously processed form submissions\")\n",
        "        else:\n",
        "            print(\"No existing profiles found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading existing profiles: {str(e)}\")\n",
        "\n",
        "    return profiles, processed_files, processed_emails\n",
        "\n",
        "def identify_new_files(all_files, processed_files):\n",
        "    \"\"\"\n",
        "    Identify new files that haven't been processed yet\n",
        "\n",
        "    Args:\n",
        "        all_files: List of all file paths\n",
        "        processed_files: Set of previously processed file paths\n",
        "\n",
        "    Returns:\n",
        "        List of file paths that haven't been processed\n",
        "    \"\"\"\n",
        "    new_files = [f for f in all_files if f not in processed_files]\n",
        "    print(f\"Found {len(new_files)} new files to process\")\n",
        "    return new_files\n",
        "\n",
        "def save_all_profiles(profiles):\n",
        "    \"\"\"\n",
        "    Save all profiles to the output summary file\n",
        "\n",
        "    Args:\n",
        "        profiles: List of profiles to save\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(OUTPUT_SUMMARY_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(profiles, f, indent=2)\n",
        "        print(f\"Saved {len(profiles)} profiles to {OUTPUT_SUMMARY_FILE}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving profiles: {str(e)}\")"
      ],
      "metadata": {
        "id": "MTTBenPbWmwH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Estimation Function\n",
        "Function to estimate runtime and cost for processing files.\n"
      ],
      "metadata": {
        "id": "Cgb5sLzIW49K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_processing(files_to_process, sample_size=5):\n",
        "    \"\"\"\n",
        "    Process a sample of files and estimate total runtime and cost\n",
        "\n",
        "    Args:\n",
        "        files_to_process: List of files to process\n",
        "        sample_size: Number of files to process for the estimate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with estimation results\n",
        "    \"\"\"\n",
        "    total_files = len(files_to_process)\n",
        "\n",
        "    if total_files == 0:\n",
        "        print(\"No files to process.\")\n",
        "        return None\n",
        "\n",
        "    # Use the minimum of sample_size or total_files\n",
        "    actual_sample_size = min(sample_size, total_files)\n",
        "\n",
        "    # Process a sample of files\n",
        "    sample_files = files_to_process[:actual_sample_size]\n",
        "    print(f\"Processing {actual_sample_size} sample files out of {total_files} total files...\")\n",
        "\n",
        "    # Track timing and token usage\n",
        "    start_time = time.time()\n",
        "    token_counts = []\n",
        "\n",
        "    # Process each file in the sample\n",
        "    results = []\n",
        "\n",
        "    for file_path in tqdm(sample_files):\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        try:\n",
        "            # Extract text from file\n",
        "            text = extract_text_from_file(file_path)\n",
        "\n",
        "            if text:\n",
        "                # Estimate token count (rough estimate: 4 chars per token)\n",
        "                estimated_tokens = len(text) // 4\n",
        "                token_counts.append(min(estimated_tokens, 15000 // 4))  # Cap at our limit\n",
        "\n",
        "                # Extract research profile\n",
        "                profile = extract_research_profile(text, file_name)\n",
        "\n",
        "                # Add file info\n",
        "                profile['file_info'] = {\n",
        "                    'file_name': file_name,\n",
        "                    'file_path': file_path,\n",
        "                    'processing_time': pd.Timestamp.now().isoformat()\n",
        "                }\n",
        "\n",
        "                # Save individual profile\n",
        "                output_file = os.path.join(OUTPUT_PATH, f\"{os.path.splitext(file_name)[0]}_profile.json\")\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(profile, f, indent=2)\n",
        "\n",
        "                results.append(profile)\n",
        "                print(f\"Processed: {file_name}\")\n",
        "            else:\n",
        "                print(f\"No text extracted from: {file_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {str(e)}\")\n",
        "\n",
        "    # Display the results\n",
        "    if len(results) > 0:\n",
        "        print(\"\\nSample profiles extracted:\")\n",
        "        display_profiles(results)\n",
        "\n",
        "    # Calculate timing\n",
        "    end_time = time.time()\n",
        "    total_sample_time = end_time - start_time\n",
        "    avg_time_per_file = total_sample_time / max(len(sample_files), 1)\n",
        "    estimated_total_time = avg_time_per_file * total_files\n",
        "\n",
        "    # Calculate estimated token usage\n",
        "    avg_tokens_per_file = sum(token_counts) / max(len(token_counts), 1) if token_counts else 0\n",
        "    estimated_total_tokens_in = avg_tokens_per_file * total_files\n",
        "\n",
        "    # Estimate output tokens (typically smaller than input)\n",
        "    estimated_tokens_out = estimated_total_tokens_in * 0.3  # Rough estimate\n",
        "\n",
        "    # Calculate cost (Claude 3.7 Sonnet pricing)\n",
        "    # As of March 2025, using approximations\n",
        "    input_cost_per_1k = 0.03  # $0.03 per 1K input tokens for Claude 3.7 Sonnet\n",
        "    output_cost_per_1k = 0.15  # $0.15 per 1K output tokens for Claude 3.7 Sonnet\n",
        "\n",
        "    estimated_input_cost = (estimated_total_tokens_in / 1000) * input_cost_per_1k\n",
        "    estimated_output_cost = (estimated_tokens_out / 1000) * output_cost_per_1k\n",
        "    estimated_total_cost = estimated_input_cost + estimated_output_cost\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PROCESSING ESTIMATE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Sample size: {len(sample_files)} files\")\n",
        "    print(f\"Total files to process: {total_files}\")\n",
        "    print(f\"\\nTiming Information:\")\n",
        "    print(f\"  Average processing time per file: {avg_time_per_file:.2f} seconds\")\n",
        "    print(f\"  Estimated total processing time: {estimated_total_time:.2f} seconds \" +\n",
        "          f\"({estimated_total_time/60:.2f} minutes, {estimated_total_time/3600:.2f} hours)\")\n",
        "\n",
        "    print(f\"\\nToken Usage Estimate:\")\n",
        "    print(f\"  Average input tokens per file: {avg_tokens_per_file:.0f}\")\n",
        "    print(f\"  Estimated total input tokens: {estimated_total_tokens_in:.0f}\")\n",
        "    print(f\"  Estimated total output tokens: {estimated_tokens_out:.0f}\")\n",
        "\n",
        "    print(f\"\\nCost Estimate:\")\n",
        "    print(f\"  Estimated input cost: ${estimated_input_cost:.2f}\")\n",
        "    print(f\"  Estimated output cost: ${estimated_output_cost:.2f}\")\n",
        "    print(f\"  Estimated total cost: ${estimated_total_cost:.2f}\")\n",
        "\n",
        "    print(\"\\nNote: These are rough estimates based on the sample. Actual values may vary.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return estimation results\n",
        "    return {\n",
        "        'sample_size': len(sample_files),\n",
        "        'total_files': total_files,\n",
        "        'avg_time_per_file': avg_time_per_file,\n",
        "        'estimated_total_time': estimated_total_time,\n",
        "        'estimated_total_time_minutes': estimated_total_time/60,\n",
        "        'estimated_total_time_hours': estimated_total_time/3600,\n",
        "        'avg_tokens_per_file': avg_tokens_per_file,\n",
        "        'estimated_total_tokens_in': estimated_total_tokens_in,\n",
        "        'estimated_total_tokens_out': estimated_tokens_out,\n",
        "        'estimated_input_cost': estimated_input_cost,\n",
        "        'estimated_output_cost': estimated_output_cost,\n",
        "        'estimated_total_cost': estimated_total_cost,\n",
        "        'sample_profiles': results\n",
        "    }"
      ],
      "metadata": {
        "id": "F3sT3S1hXCuf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Full Processing Function\n",
        "Function to process all files or only new files."
      ],
      "metadata": {
        "id": "eNZ9lY-fXGt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files(files_to_process, progress_update=None):\n",
        "    \"\"\"\n",
        "    Process a list of files and extract research profiles\n",
        "\n",
        "    Args:\n",
        "        files_to_process: List of files to process\n",
        "        progress_update: Optional callback function for progress updates\n",
        "\n",
        "    Returns:\n",
        "        List of extracted profiles\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total = len(files_to_process)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, file_path in enumerate(tqdm(files_to_process)):\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        try:\n",
        "            # Extract text from file\n",
        "            text = extract_text_from_file(file_path)\n",
        "\n",
        "            if text:\n",
        "                # Extract research profile\n",
        "                profile = extract_research_profile(text, file_name)\n",
        "\n",
        "                # Add file info\n",
        "                profile['file_info'] = {\n",
        "                    'file_name': file_name,\n",
        "                    'file_path': file_path,\n",
        "                    'processing_time': pd.Timestamp.now().isoformat()\n",
        "                }\n",
        "\n",
        "                # Save individual profile\n",
        "                output_file = os.path.join(OUTPUT_PATH, f\"{os.path.splitext(file_name)[0]}_profile.json\")\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(profile, f, indent=2)\n",
        "\n",
        "                results.append(profile)\n",
        "                print(f\"Processed ({i+1}/{total}): {file_name}\")\n",
        "            else:\n",
        "                print(f\"No text extracted from: {file_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {str(e)}\")\n",
        "\n",
        "        # Provide progress update if callback is provided\n",
        "        if progress_update:\n",
        "            progress_update(i+1, total)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"\\nProcessing complete! Processed {len(results)} files in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Average time per file: {elapsed_time/max(len(files_to_process), 1):.2f} seconds\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "WF7oFz_UXGXK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Main Execution Functions\n",
        "Functions to run the profile extraction process with different options."
      ],
      "metadata": {
        "id": "d_zuUmp0XVV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confirm_action(message):\n",
        "    \"\"\"\n",
        "    Ask for user confirmation before proceeding\n",
        "\n",
        "    Args:\n",
        "        message: Message to display\n",
        "\n",
        "    Returns:\n",
        "        True if user confirms, False otherwise\n",
        "    \"\"\"\n",
        "    response = input(f\"{message} (y/n): \").strip().lower()\n",
        "    return response == 'y' or response == 'yes'\n",
        "\n",
        "def run_profile_extraction(run_updates_only=True, sample_size=5, run_estimation=True, include_form_responses=True):\n",
        "    \"\"\"\n",
        "    Run the profile extraction process\n",
        "    \"\"\"\n",
        "    print(f\"Running profile extraction with updates_only={run_updates_only}, include_form_responses={include_form_responses}\")\n",
        "\n",
        "    # Scan all directories for files\n",
        "    all_files = scan_all_directories(INPUT_DIRS)\n",
        "\n",
        "    if run_updates_only:\n",
        "        # Load existing profiles and identify new files\n",
        "        existing_profiles, processed_files, processed_emails = load_existing_profiles()\n",
        "        files_to_process = identify_new_files(all_files, processed_files)\n",
        "\n",
        "        if not files_to_process and not include_form_responses:\n",
        "            print(\"No new files to process.\")\n",
        "            return existing_profiles\n",
        "    else:\n",
        "        # Confirm overwrite\n",
        "        if os.path.exists(OUTPUT_SUMMARY_FILE):\n",
        "            if not confirm_action(\"This will reprocess all files and overwrite existing profiles. Continue?\"):\n",
        "                print(\"Operation cancelled.\")\n",
        "                return None\n",
        "\n",
        "        # Process all files\n",
        "        existing_profiles = []\n",
        "        processed_emails = set()\n",
        "        files_to_process = all_files\n",
        "\n",
        "    # Process files if needed\n",
        "    new_profiles = []\n",
        "    if files_to_process:\n",
        "        # Run estimation if requested\n",
        "        if run_estimation:\n",
        "            estimation_results = estimate_processing(files_to_process, sample_size)\n",
        "\n",
        "            if estimation_results:\n",
        "                # Ask for confirmation before proceeding with full processing\n",
        "                if not confirm_action(f\"Estimated processing time: {estimation_results['estimated_total_time_minutes']:.2f} minutes\\nEstimated cost: ${estimation_results['estimated_total_cost']:.2f}\\n\\nProceed with processing {len(files_to_process)} files?\"):\n",
        "                    print(\"File processing cancelled.\")\n",
        "                    files_to_process = []\n",
        "\n",
        "        # Process files\n",
        "        if files_to_process:\n",
        "            new_profiles = process_files(files_to_process)\n",
        "\n",
        "    # Combine new profiles with existing ones (if in update mode)\n",
        "    all_profiles = existing_profiles + new_profiles\n",
        "\n",
        "    # Process form responses if requested\n",
        "    if include_form_responses:\n",
        "        print(\"\\nProcessing text-based form responses...\")\n",
        "        form_profiles = process_form_responses(\n",
        "            FORM_RESPONSES_SHEET_ID,\n",
        "            FORM_RESPONSES_RANGE,\n",
        "            EMAIL_COLUMN,\n",
        "            FIRST_NAME_COLUMN,\n",
        "            LAST_NAME_COLUMN,\n",
        "            processed_emails if run_updates_only else None  # Only pass if doing incremental update\n",
        "        )\n",
        "\n",
        "        # Combine with file-based profiles\n",
        "        all_profiles = all_profiles + form_profiles\n",
        "\n",
        "    # Save all profiles\n",
        "    if new_profiles or (include_form_responses and len(form_profiles) > 0):\n",
        "        save_all_profiles(all_profiles)\n",
        "\n",
        "    print(f\"Profile extraction complete. Total profiles: {len(all_profiles)}\")\n",
        "    return all_profiles"
      ],
      "metadata": {
        "id": "N1GxQx4_XX5g"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Interactive Run\n",
        "Use this section to run the profile extraction process interactively.\n"
      ],
      "metadata": {
        "id": "_odfEmyoXfZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters for the run\n",
        "run_updates_only = True  # Set to False to reprocess all files\n",
        "include_form_responses = True  # Set to False to skip form responses\n",
        "sample_size = 5  # Number of files to use for estimation\n",
        "run_estimation = False  # Set to False to skip estimation\n",
        "\n",
        "# Run the profile extraction process\n",
        "profiles = run_profile_extraction(\n",
        "    run_updates_only=run_updates_only,\n",
        "    sample_size=sample_size,\n",
        "    run_estimation=run_estimation,\n",
        "    include_form_responses=include_form_responses\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "80dab79d8a454d7d84a32a32585a794b",
            "e36774101a1d4d698b6b796398706c2c",
            "9f835613d90e4c31a5b26e7b15b13279",
            "9db2d09dfef84865945c79ff92b01875",
            "66315d45739147d39d90547e1056606b",
            "2ac3b5dc7f804398801fc51830557aea",
            "f5fa9e540c014b48b71c7254f9fba549",
            "573b3ceb471e4fbfb160a336a5aedcd7",
            "f3c38be5d53147aa80e139a524b15c5d",
            "66027f444241400eac14c4667787db00",
            "434b6f04f7ca43b0ae1d5d1b0f5c2bb2"
          ]
        },
        "collapsed": true,
        "id": "v_PPOl3CXkhJ",
        "outputId": "555d940c-8358-46b0-c59e-c37cd8100f3c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running profile extraction with updates_only=True, include_form_responses=True\n",
            "Scanning directory: /content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research\n",
            "Found 152 files\n",
            "Scanning directory: /content/drive/My Drive/Data Science/Symposium/Recent_CV_uploads\n",
            "Found 27 files\n",
            "Total files found: 179\n",
            "Loaded 256 existing profiles\n",
            "Found 177 previously processed files\n",
            "Found 79 previously processed form submissions\n",
            "Found 2 new files to process\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80dab79d8a454d7d84a32a32585a794b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error extracting DOCX /content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research/Ryan Patrick - Sr. Director of Analytics - Ryan Patrick.doc: Package not found at '/content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research/Ryan Patrick - Sr. Director of Analytics - Ryan Patrick.doc'\n",
            "No text extracted from: Ryan Patrick - Sr. Director of Analytics - Ryan Patrick.doc\n",
            "Error extracting DOCX /content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research/Golann_CV_Jan 2025 for website - Joanne Golann.doc: file '/content/drive/My Drive/Data Science/Symposium/AI Days Resumes_Research/Golann_CV_Jan 2025 for website - Joanne Golann.doc' is not a Word file, content type is 'application/vnd.openxmlformats-officedocument.themeManager+xml'\n",
            "No text extracted from: Golann_CV_Jan 2025 for website - Joanne Golann.doc\n",
            "\n",
            "Processing complete! Processed 0 files in 0.87 seconds\n",
            "Average time per file: 0.44 seconds\n",
            "\n",
            "Processing text-based form responses...\n",
            "Skipping 79 already processed form submissions\n",
            "Loading form responses from sheet ID: 1347yg-cKPv3VWmXSg6M7X-AI4QDCDETQUBPaxXWpfmc\n",
            "Error loading form responses: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google_auth_httplib2._Response object at 0x79f06d4b5f50>)\n",
            "Profile extraction complete. Total profiles: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Results\n",
        "Execute this cell to display the extracted profiles.\n",
        "\n",
        "# Display the extracted profiles (limit to 10 for readability)\n",
        "if profiles:\n",
        "    display_profiles(profiles, limit=10)\n",
        "\n",
        "# Section 6: Future Sections (Placeholders)\n",
        "The following sections will be implemented in future iterations.\n",
        "\n",
        "## 6.1 Collaboration Analysis\n",
        "This section will analyze the extracted profiles to identify potential collaborations.\n",
        "\n",
        "TODO: Implement collaboration matching algorithms.\n",
        "\n",
        "## 6.2 Email Notifications\n",
        "This section will generate and send email notifications to participants.\n",
        "TODO: Implement email notification functionality.\n",
        "\n",
        "## 6.3 Advanced Visualization\n",
        "This section will provide advanced visualizations of the research network.\n",
        "TODO: Implement research network visualization."
      ],
      "metadata": {
        "id": "4fWDC6yrxPOR"
      }
    }
  ]
}